{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f7bc6d6-4adb-4f3f-add4-47d77810135f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a204f8-fff6-4ef4-9687-339510dd512f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## =============== ##\n",
    "## Define our data ##\n",
    "## =============== ##\n",
    "\n",
    "# input to our model. Represents time in seconds\n",
    "x_data = np.array([0,1,2]).reshape(3,1)\n",
    "# outputs associated to each input. Represents cantidad de lluvia in mm^3\n",
    "t_data = np.array([0.2,1.3,2.4]).reshape(3,1)\n",
    "\n",
    "## display\n",
    "plt.plot(x_data,t_data,'o', markersize = 8, label = 'data observations')\n",
    "plt.xlabel('tiempo')\n",
    "plt.ylabel('cantidad de lluvia')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f748162f-fe6a-4669-b451-890b35381be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ======================================================================= ##\n",
    "## display possible functions depending on different values of $w$ and $b$ ##\n",
    "## ======================================================================= ##\n",
    "\n",
    "## fix seed so that randomness is controlled.\n",
    "np.random.seed(5)\n",
    "\n",
    "## number of points in the domain used to plot the functions \n",
    "N_points_domain = 100\n",
    "\n",
    "## display data again\n",
    "plt.plot(x_data,t_data,'o', markersize = 8, label = 'data observations')\n",
    "plt.xlabel('tiempo')\n",
    "plt.ylabel('cantidad de lluvia')\n",
    "\n",
    "## function implementing an activation function\n",
    "def activation_function_linear(x):\n",
    "    return x\n",
    "\n",
    "## function that implements the computational graph\n",
    "def computation_graph_linear(x,w,b):\n",
    "    ''' This function represents a computational graph, a neural network, that implements a linear operation'''\n",
    "    # this is the W^0 x from the theory above implemented using a transposition ;)\n",
    "    y = activation_function_linear(np.matmul(x,w) + b)\n",
    "    return y\n",
    "\n",
    "## function that implements the computational graph\n",
    "def computation_graph_linear_just_weight(x,w):\n",
    "    ''' This function represents a computational graph, a neural network, that implements a linear operation, with no weight'''\n",
    "    # this is the W^0 x from the theory above implemented using a transposition ;)\n",
    "    y = activation_function_linear(np.matmul(x,w))\n",
    "    return y\n",
    "\n",
    "## function that initializes the values of a computational graph\n",
    "def create_computation_graph_linear(n_in,n_out):\n",
    "    ''' Create elements of the computational graph'''\n",
    "    # parameters\n",
    "    w = np.random.randn(n_in,n_out) + 1 # get a random value from standard normal distribution\n",
    "    b = np.random.randn(n_out,)*5 # get a random value from Gaussian with mean 0 and standard deviation 5.\n",
    "\n",
    "    return w,b\n",
    "\n",
    "## ===========================================\n",
    "## Neural network specification for each layer\n",
    "\n",
    "# neurons of input layer\n",
    "n_in = 1\n",
    "# neurons of output layer\n",
    "n_out = 1\n",
    "\n",
    "## ================================================================================\n",
    "## Create several possible functions that our specific neural network can implement\n",
    "for i in range(10):\n",
    "\n",
    "    # domain over where we want to plot the function implemented by the NNet\n",
    "    x_range = np.linspace(-1,4, N_points_domain).reshape((N_points_domain,1))\n",
    "\n",
    "    # initialize one of our networks\n",
    "    w, b = create_computation_graph_linear(n_in,n_out)\n",
    "\n",
    "    # projection from input x to output y through computational graph\n",
    "    y_range = computation_graph_linear(x_range,w,b)\n",
    "\n",
    "    # check how this computational_graph predicts at the inputs denote by our observed data X.\n",
    "    y_pred = computation_graph_linear(x_data,w,b)\n",
    "\n",
    "    if i == 0:\n",
    "        plt.plot(x_data, y_pred,'*', markersize = 5, color = f\"C{i+1}\", label = 'Network predictions at training input data')\n",
    "        plt.plot(x_range,y_range, color = f\"C{i+1}\", label = 'NeuralNet implemented function on all the domain' )\n",
    "    else:\n",
    "        plt.plot(x_data, y_pred,'*', markersize = 5,  color = f\"C{i+1}\")\n",
    "        plt.plot(x_range,y_range, color = f\"C{i+1}\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6acb680-1e3f-440b-918b-581c06038d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ====================================== ##\n",
    "## display loss of each possible function ##\n",
    "## ====================================== ##\n",
    "## Let's see the associated loss to each possible function and each prediction of this function at the training points.\n",
    "## We show two different losses: squared (top) and absolute ( bottom )\n",
    "## I repeat code from above but computing and plotting the loss.\n",
    "\n",
    "## function implementing squared loss function\n",
    "def squared_loss_function(y_pred,t):\n",
    "    return (y_pred-t)**2\n",
    "\n",
    "def absolute_loss_function(y_pred,t):\n",
    "    return np.abs(y_pred-t)\n",
    "\n",
    "## fix seed so that randomness is controlled.\n",
    "np.random.seed(5)\n",
    "\n",
    "## number of points in the domain used to plot the functions \n",
    "N_points_domain = 100\n",
    "\n",
    "## display data again\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.plot(x_data,t_data,'o', markersize = 8, label = 'data observations')\n",
    "plt.xlabel('tiempo')\n",
    "plt.ylabel('cantidad de lluvia')\n",
    "\n",
    "## ===========================================\n",
    "## Neural network specification for each layer\n",
    "\n",
    "# neurons of input layer\n",
    "n_in = 1\n",
    "# neurons of output layer\n",
    "n_out = 1\n",
    "\n",
    "## ================================================================================\n",
    "## Create several possible functions that our specific neural network can implement\n",
    "for i in range(3):\n",
    "\n",
    "    # domain over where we want to plot the function implemented by the NNet\n",
    "    x_range = np.linspace(-1,4, N_points_domain).reshape((N_points_domain,1))\n",
    "\n",
    "    # initialize one of our networks\n",
    "    w, b = create_computation_graph_linear(n_in,n_out)\n",
    "\n",
    "    # projection from input x to output y through computational graph\n",
    "    y_range = computation_graph_linear(x_range,w,b)\n",
    "\n",
    "    # check how this computational_graph predicts at the inputs denote by our observed data X.\n",
    "    y_pred = computation_graph_linear(x_data,w,b)\n",
    "\n",
    "    # compute the two losses at the predictions\n",
    "    squared_loss = squared_loss_function(y_pred, t_data)\n",
    "    absolute_loss = absolute_loss_function(y_pred, t_data)\n",
    "\n",
    "    if i == 0:\n",
    "        plt.plot(x_data, y_pred,'*', markersize = 7, color = f\"C{i+1}\", label = 'Network predictions at training input data')\n",
    "        plt.plot(x_range,y_range, color = f\"C{i+1}\", label = 'NeuralNet implemented function on all the domain' )\n",
    "      \n",
    "    else:\n",
    "        plt.plot(x_data, y_pred,'*', markersize = 7,  color = f\"C{i+1}\")\n",
    "        plt.plot(x_range,y_range, color = f\"C{i+1}\")\n",
    "\n",
    "    ## plot squared loss associated at each point\n",
    "    for xi, yi, sl in zip(x_data,y_pred,squared_loss):\n",
    "        plt.text(xi, yi, f'{sl[0]:.2f}', fontsize=12, va='top', color = f\"C{i+1}\" ) \n",
    "\n",
    "    ## plot absolute loss associated at each point\n",
    "    for xi, yi, sl in zip(x_data,y_pred,absolute_loss):\n",
    "        plt.text(xi, yi, f'{sl[0]:.2f}', fontsize=12, va='bottom', color = f\"C{i+1}\" ) \n",
    "\n",
    "    # draw line between dots to highliht what the loss measures\n",
    "    for xi,t_d,y_p in zip(x_data,t_data,y_pred):\n",
    "        plt.plot([xi,xi], [t_d, y_p], '--',color = f\"C{i+1}\", alpha = 0.5)\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faa971f-e61e-4551-be04-d60dbb838d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ======================================================================== ##\n",
    "## display loss as a function of parameters (loss incurred by each network) ##\n",
    "## ======================================================================== ##\n",
    "## Let's see the associated loss to each possible function but seeing the loss\n",
    "## as a function of the parameters.\n",
    "## We show two different losses: squared (top) and absolute ( bottom )\n",
    "## I repeat code from above but computing and plotting the loss.\n",
    "\n",
    "## function implementing squared loss function\n",
    "def squared_loss_function(y_pred,t):\n",
    "    return (y_pred-t)**2\n",
    "\n",
    "def absolute_loss_function(y_pred,t):\n",
    "    return np.abs(y_pred-t)\n",
    "\n",
    "## fix seed so that randomness is controlled.\n",
    "np.random.seed(5)\n",
    "\n",
    "## number of points in the domain used to plot the functions \n",
    "N_points_domain = 100\n",
    "\n",
    "## display data again\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.plot(x_data,t_data,'o', markersize = 8, label = 'data observations', zorder = 200)\n",
    "plt.xlabel('tiempo')\n",
    "plt.ylabel('cantidad de lluvia')\n",
    "\n",
    "## ===========================================\n",
    "## Neural network specification for each layer\n",
    "\n",
    "# neurons of input layer\n",
    "n_in = 1\n",
    "# neurons of output layer\n",
    "n_out = 1\n",
    "\n",
    "## ================================================================================\n",
    "## Create several possible functions that our specific neural network can implement\n",
    "squared_loss_acc = []\n",
    "absolute_loss_acc = []\n",
    "w_acc = []\n",
    "for i in range(50):\n",
    "\n",
    "    # domain over where we want to plot the function implemented by the NNet\n",
    "    x_range = np.linspace(-1,4, N_points_domain).reshape((N_points_domain,1))\n",
    "\n",
    "    # initialize one of our networks\n",
    "    w, b = create_computation_graph_linear(n_in,n_out)\n",
    "\n",
    "    # projection from input x to output y through computational graph\n",
    "    y_range = computation_graph_linear_just_weight(x_range,w)\n",
    "\n",
    "    # check how this computational_graph predicts at the inputs denote by our observed data X.\n",
    "    y_pred = computation_graph_linear_just_weight(x_data,w)\n",
    "\n",
    "    # compute the two losses at the predictions\n",
    "    squared_loss = squared_loss_function(y_pred, t_data)\n",
    "    absolute_loss = absolute_loss_function(y_pred, t_data)\n",
    "\n",
    "    # acumulate loss and parameter used\n",
    "    squared_loss_acc.append(np.sum(squared_loss))\n",
    "    absolute_loss_acc.append(np.sum(absolute_loss))\n",
    "    w_acc.append(np.squeeze(w))\n",
    "\n",
    "    if i == 0:\n",
    "        plt.plot(x_data, y_pred,'*', markersize = 7, color = f\"C{i+1}\", label = 'Network predictions at training input data')\n",
    "        plt.plot(x_range,y_range, color = f\"C{i+1}\", label = 'NeuralNet implemented function on all the domain' )\n",
    "      \n",
    "    else:\n",
    "        plt.plot(x_data, y_pred,'*', markersize = 7,  color = f\"C{i+1}\")\n",
    "        plt.plot(x_range,y_range, color = f\"C{i+1}\")\n",
    "\n",
    "    # plot loss associated to just first three lines to see it better.\n",
    "    if i == 0:\n",
    "        \n",
    "        ## plot squared loss associated at each point\n",
    "        for xi, yi, sl in zip(x_data,y_pred,squared_loss):\n",
    "            plt.text(xi, yi, f'{sl[0]:.2f}', fontsize=12, va='top', color = f\"C{i+1}\" ) \n",
    "            \n",
    "        ## plot absolute loss associated at each point\n",
    "        for xi, yi, sl in zip(x_data,y_pred,absolute_loss):\n",
    "            plt.text(xi, yi, f'{sl[0]:.2f}', fontsize=12, va='bottom', color = f\"C{i+1}\" ) \n",
    "    \n",
    "        # draw line between dots to highliht what the loss measures\n",
    "        for xi,t_d,y_p in zip(x_data,t_data,y_pred):\n",
    "            plt.plot([xi,xi], [t_d, y_p], '--',color = f\"C{i+1}\", alpha = 0.5)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "# sort w_acc\n",
    "idx = np.argsort(w_acc)\n",
    "\n",
    "plt.plot(np.array(w_acc)[idx], np.array(squared_loss_acc)[idx])\n",
    "plt.plot(np.array(w_acc)[idx], np.array(squared_loss_acc)[idx],'*')\n",
    "plt.xlabel('weight values')\n",
    "plt.ylabel('squared loss function')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.array(w_acc)[idx], np.array(absolute_loss_acc)[idx])\n",
    "plt.plot(np.array(w_acc)[idx], np.array(absolute_loss_acc)[idx],'*')\n",
    "plt.xlabel('weight values')\n",
    "plt.ylabel('absolute loss function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a22d809-329a-453a-8774-62192a7edee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c45044-97f7-465d-be9a-2d11cfd20f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio en clase\n",
    "\n",
    "## =============== ##\n",
    "## Define our data ##\n",
    "## =============== ##\n",
    "\n",
    "# input to our model. Represents pixel\n",
    "x_data = np.array([0, 1.5, 2, 2.5, 3, 4, 5]).reshape(7, 1)\n",
    "# outputs associated to each input. Represents etiqueta de clase\n",
    "t_data = np.array([0, 0, 0, 0, 1, 1, 1]).reshape(7, 1)\n",
    "\n",
    "## display\n",
    "plt.plot(x_data[t_data > 0], t_data[t_data > 0], '*', markersize=8, label='data observations (naranja)', color='orange')\n",
    "plt.plot(x_data[t_data == 0], t_data[t_data == 0], 'o', markersize=8, label='data observations (azul)', color='blue')\n",
    "plt.xlabel('imagen')\n",
    "plt.ylabel('etiqueta de clase')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c2a252-1db5-4e3d-8f7c-744b1b4237bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo lineal sigmoide\n",
    "\n",
    "## ======================================================================= ##\n",
    "## display possible functions depending on different values of $w$ and $b$ ##\n",
    "## ======================================================================= ##\n",
    "\n",
    "## fix seed so that randomness is controlled.\n",
    "np.random.seed(5)\n",
    "\n",
    "## number of points in the domain used to plot the functions \n",
    "N_points_domain = 100\n",
    "\n",
    "# input to our model. Represents pixel\n",
    "x_data = np.array([0, 1.5, 2, 2.5, 3, 4, 5]).reshape(7, 1)\n",
    "# outputs associated to each input. Represents etiqueta de clase\n",
    "t_data = np.array([0, 0, 0, 0, 1, 1, 1]).reshape(7, 1)\n",
    "\n",
    "## display\n",
    "plt.plot(x_data[t_data > 0], t_data[t_data > 0], '*', markersize=8, label='data observations (naranja)', color='orange')\n",
    "plt.plot(x_data[t_data == 0], t_data[t_data == 0], 'o', markersize=8, label='data observations (azul)', color='blue')\n",
    "plt.xlabel('imagen')\n",
    "plt.ylabel('etiqueta de clase')\n",
    "plt.legend()\n",
    "\n",
    "## function implementing an activation function\n",
    "def activation_function_sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "## function that implements the computational graph\n",
    "def computation_graph_sigmoid(x,w,b):\n",
    "    ''' This function represents a computational graph, a neural network, that implements a linear operation'''\n",
    "    # this is the W^0 x from the theory above implemented using a transposition ;)\n",
    "    y = activation_function_sigmoid(np.matmul(x,w) + b)\n",
    "    return y\n",
    "\n",
    "## function that implements the computational graph\n",
    "def computation_graph_sigmoid_just_weight(x,w):\n",
    "    ''' This function represents a computational graph, a neural network, that implements a linear operation, with no weight'''\n",
    "    # this is the W^0 x from the theory above implemented using a transposition ;)\n",
    "    y = activation_function_sigmoid(np.matmul(x,w))\n",
    "    return y\n",
    "\n",
    "## function that initializes the values of a computational graph\n",
    "def create_computation_graph_sigmoid(n_in,n_out):\n",
    "    ''' Create elements of the computational graph'''\n",
    "    # parameters\n",
    "    w = np.random.randn(n_in,n_out) + 1 # get a random value from standard normal distribution\n",
    "    b = np.random.randn(n_out,)*5 # get a random value from Gaussian with mean 0 and standard deviation 5.\n",
    "\n",
    "    return w,b\n",
    "\n",
    "## ===========================================\n",
    "## Neural network specification for each layer\n",
    "\n",
    "# neurons of input layer\n",
    "n_in = 1\n",
    "# neurons of output layer\n",
    "n_out = 1\n",
    "\n",
    "## ================================================================================\n",
    "## Create several possible functions that our specific neural network can implement\n",
    "for i in range(10):\n",
    "\n",
    "    # domain over where we want to plot the function implemented by the NNet\n",
    "    x_range = np.linspace(-1,6, N_points_domain).reshape((N_points_domain,1))\n",
    "\n",
    "    # initialize one of our networks\n",
    "    w, b = create_computation_graph_sigmoid(n_in,n_out)\n",
    "\n",
    "    # projection from input x to output y through computational graph\n",
    "    y_range = computation_graph_sigmoid(x_range,w,b)\n",
    "\n",
    "    # check how this computational_graph predicts at the inputs denote by our observed data X.\n",
    "    y_pred = computation_graph_sigmoid(x_data,w,b)\n",
    "\n",
    "    if i == 0:\n",
    "        plt.plot(x_data, y_pred,'*', markersize = 5, color = f\"C{i+1}\", label = 'Network predictions at training input data')\n",
    "        plt.plot(x_range,y_range, color = f\"C{i+1}\", label = 'NeuralNet implemented function on all the domain' )\n",
    "    else:\n",
    "        plt.plot(x_data, y_pred,'*', markersize = 5,  color = f\"C{i+1}\")\n",
    "        plt.plot(x_range,y_range, color = f\"C{i+1}\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3697c289-953e-477d-bdab-eb72177cb6d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 13.390625  ,  10.0003061 ,   7.40685344,   5.47989055,\n",
       "         4.1006494 ,   3.16140873,   2.56494446,   2.22399191,\n",
       "         2.06072006,   2.00621776,   1.99999179,   1.98947699,\n",
       "         1.92955829,   1.78210468,   1.51551511,   1.10427641,\n",
       "         0.52853311,  -0.22633082,  -1.17009821,  -2.30811303,\n",
       "        -3.64166276,  -5.16834882,  -6.88244506,  -8.77524426,\n",
       "       -10.83539275, -13.04921299, -15.40101431, -17.87339159,\n",
       "       -20.44751207, -23.10339017, -25.82015039, -28.57627822,\n",
       "       -31.34985916, -34.11880573, -36.86107254, -39.55485949,\n",
       "       -42.17880289, -44.71215475, -47.13495001, -49.42816197,\n",
       "       -51.57384558, -53.55526898, -55.3570329 , -56.96517828,\n",
       "       -58.3672818 , -59.5525396 , -60.5118389 , -61.23781777,\n",
       "       -61.72491298, -61.96939576, -61.96939576, -61.72491298,\n",
       "       -61.23781777, -60.5118389 , -59.5525396 , -58.3672818 ,\n",
       "       -56.96517828, -55.3570329 , -53.55526898, -51.57384558,\n",
       "       -49.42816197, -47.13495001, -44.71215475, -42.17880289,\n",
       "       -39.55485949, -36.86107254, -34.11880573, -31.34985916,\n",
       "       -28.57627822, -25.82015039, -23.10339017, -20.44751207,\n",
       "       -17.87339159, -15.40101431, -13.04921299, -10.83539275,\n",
       "        -8.77524426,  -6.88244506,  -5.16834882,  -3.64166276,\n",
       "        -2.30811303,  -1.17009821,  -0.22633082,   0.52853311,\n",
       "         1.10427641,   1.51551511,   1.78210468,   1.92955829,\n",
       "         1.98947699,   1.99999179,   2.00621776,   2.06072006,\n",
       "         2.22399191,   2.56494446,   3.16140873,   4.1006494 ,\n",
       "         5.47989055,   7.40685344,  10.0003061 ,  13.390625  ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Definición de rango\n",
    "\n",
    "N_points_domain = 100\n",
    "\n",
    "x_range = np.linspace(-2.5,2.5, N_points_domain)\n",
    "\n",
    "def pintar_funcion(x):\n",
    "    result = (x**2-4)**3 + 2\n",
    "    return result\n",
    "pintar_funcion(x_range)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (deep_learning)",
   "language": "python",
   "name": "deep_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
